# ğŸ™ï¸ Real-Time Multilingual Speech Translator

This project was developed as part of my AI research at the University of Florida. It performs real-time speech-to-speech translation using open-source models for ASR (Whisper), machine translation (M2M100), and TTS (TTS by Coqui). The goal was to explore low-latency multilingual communication for applications in global accessibility, human-computer interaction, and real-time dialogue systems.

The final system captures live audio from a microphone, transcribes it to text, translates it into a target language, and synthesizes natural-sounding speech in under 1.5 seconds.

[![CI](https://github.com/gebrilfradj/realtime-speech-translator/actions/workflows/ci.yml/badge.svg)](https://github.com/gebrilfradj/realtime-speech-translator/actions/workflows/ci.yml) 
---

## ğŸš€ Features

- ğŸ§ Live microphone input recording
- ğŸ§  Speech-to-text using OpenAI Whisper
- ğŸŒ Translation using Facebook's M2M100 multilingual model
- ğŸ”Š Text-to-speech output via Coqui TTS
- ğŸ“Š Benchmarking script to measure average end-to-end latency
- ğŸ§µ Multi-threaded real-time pipeline
- ğŸ§ª Sample generation utility

---

## ğŸ“¦ Tech Stack

- **Speech Recognition (ASR):** [OpenAI Whisper](https://github.com/openai/whisper)
- **Machine Translation:** [Facebook M2M100 (418M)](https://huggingface.co/facebook/m2m100_418M)
- **Text-to-Speech (TTS):** [Coqui TTS](https://github.com/coqui-ai/TTS)
- **Audio Playback & Recording:** PyAudio, PyDub
- **Hardware**: Tested on RTX 3050 Ti (laptop)  


---

## ğŸ“‚ Project Structure

- â”œâ”€â”€ main.py # Real-time pipeline (mic â†’ translation â†’ speech)
- â”œâ”€â”€ benchmark.py # Measures average latency for translation loop
- â”œâ”€â”€ asr.py # ASR model loader and transcription
- â”œâ”€â”€ translate.py # Text translation functions
- â”œâ”€â”€ tts.py # Text-to-speech synthesis
- â”œâ”€â”€ audio_utils.py # Audio recording & WAV file handling
- â”œâ”€â”€ config.py # Global constants (chunk size, sampling rate, etc.)
- â”œâ”€â”€ make_sample.py # Optional: Generate sample audio for testing
- â”œâ”€â”€ requirements.txt # Dependency list
- â””â”€â”€ .gitignore

---

## ğŸ§ª Quickstart

### 1. Clone the repo

- git clone
- cd speech_translate

### 2. Set up the environment

- python -m venv .venv
- source .venv/bin/activate  # or .venv\Scripts\activate on Windows
- pip install -r requirements.txt

### 3. Run real-time translation

- python main.py --tgt es   # Example: translate from any language to Spanish

### 4. Run benchmark on sample audio

- python benchmark.py --audio sample.wav --src en --tgt fr
- (Optional: use make_sample.py to generate sample.wav.)
- Example Output: Avg latency: 1.41s on NVIDIA GeForce RTX 3050 Ti Laptop GPU

---

### ğŸŒ Supported Languages
Update LANGUAGE_TTS_MODELS in config.py to expand.
Supported by default:

- en â€“ English

- es â€“ Spanish

- fr â€“ French

### ğŸ¯ Use Cases
- AI demo for real-time language translation

- Offline translator for travel or accessibility

- Showcase of end-to-end speech pipeline using open-source AI models

### ğŸ§  Citation
Based on research by:
Fradj et al., Real-time Multilingual Speech Translation with Open-Source Models,
UF Journal of Undergraduate Research, Spring 2025.

### ğŸ“Œ Notes
- Performance is optimized for systems with a GPU, but CPU fallback is supported.

- Language support depends on the pre-trained models used (Whisper, M2M100, Coqui TTS).


